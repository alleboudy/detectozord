{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplot for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#to see the plots inside of the notebook\n",
    "%matplotlib inline\n",
    "# import tensorflow and required layers\n",
    "# note that tensorflow.contrib.layers was previously migrated from TF Slim.\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "from scipy.stats import entropy\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_dir='./model/'\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract model tar file.\"\"\"\n",
    "  dest_directory = model_dir\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "  # Creates graph from saved graph_def.pb.\n",
    "  with tf.gfile.FastGFile(os.path.join(model_dir\n",
    "      , 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_inference_on_image(image):\n",
    "  \"\"\"Runs inference on an image.\n",
    "\n",
    "  Args:\n",
    "    image: Image file name.\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Some useful tensors:\n",
    "    # 'softmax:0': A tensor containing the normalized prediction across\n",
    "    #   1000 labels.\n",
    "    # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\n",
    "    #   float description of the image.\n",
    "    # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\n",
    "    #   encoding of the image.\n",
    "    # Runs the softmax tensor by feeding the image_data as input to the graph.\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n",
    "    predictions = sess.run(softmax_tensor,\n",
    "                           {'DecodeJpeg:0': image})\n",
    "    predictions = np.squeeze(predictions)\n",
    "    # Creates node ID --> English string lookup.\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "imgsNames = ['sax','bird']\n",
    "imgsDir='./images/'\n",
    "imgsType='.jpg'\n",
    "imgs = [misc.imread(imgsDir+imgName+imgsType) for imgName in imgsNames ]\n",
    "#img = misc.imread('images/sax.jpg')#,size=[600,600]#.reshape([1,600,600,3])\n",
    "boxWidth=100\n",
    "boxHeight=100\n",
    "xStep=int(boxWidth/2)\n",
    "yStep=int(boxHeight/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def DoTheThing(img):\n",
    "    #we slide on the picture with a small window, get the network ouput for the modified pic and record the KL divergence\n",
    "    #between the network output of the original image and the modified image at each window\n",
    "    log=dict()\n",
    "    OriginalPredictions = run_inference_on_image(img)\n",
    "    #copy = img.copy()\n",
    "    maxEnt=0\n",
    "    minEnt=1000\n",
    "    for x in np.arange(0, img.shape[1], xStep):\n",
    "        for y in np.arange(0, img.shape[0], xStep):\n",
    "            s=img[x:x+boxWidth,y:y+boxHeight,:]\n",
    "            cop=img.copy()\n",
    "            cop[x:x+boxWidth,y:y+boxHeight,:]=np.zeros_like(s)\n",
    "            predictions = run_inference_on_image(cop)\n",
    "            #print(predictions)\n",
    "            ent=entropy(OriginalPredictions,predictions)\n",
    "            print('(x,y): ',(x,y),' Entropy: ',ent)\n",
    "            #copy[x:x+boxWidth,y:y+boxHeight,:]=copy[x:x+boxWidth,y:y+boxHeight,:]*ent*255\n",
    "            maxEnt = max(maxEnt,ent)\n",
    "            minEnt = min(minEnt,ent)\n",
    "            log[(x,y)]=ent\n",
    "    log = {k: (v-minEnt)/(maxEnt-minEnt) for k, v in log.items()}#normalizing the divergences range to [0->1]\n",
    "    return log\n",
    "    #plt.imshow(copy)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img =imgs[0]#sax\n",
    "slog = DoTheThing(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img = imgs[0]\n",
    "copy = np.zeros_like(img)\n",
    "for k,v in slog.items():\n",
    "    copy[k[0]:k[0]+boxWidth,k[1]:k[1]+boxHeight,:]=copy[k[0]:k[0]+boxWidth,k[1]:k[1]+boxHeight,:]+v*100\n",
    "print('sax heatmap')\n",
    "plt.imshow(copy)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(img+copy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = imgs[1]\n",
    "blog = DoTheThing(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = imgs[1]\n",
    "copy = np.zeros_like(img)\n",
    "for k,v in blog.items():\n",
    "    copy[k[0]:k[0]+boxWidth,k[1]:k[1]+boxHeight,:]=copy[k[0]:k[0]+boxWidth,k[1]:k[1]+boxHeight,:]+v*100\n",
    "print('bird heatmap')\n",
    "plt.imshow(copy)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(img+copy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# node_lookup = NodeLookup()\n",
    "\n",
    "# top_k = predictions.argsort()[-5:][::-1]\n",
    "# for node_id in top_k:\n",
    "#   human_string = node_lookup.id_to_string(node_id)\n",
    "#   score = predictions[node_id]\n",
    "#   print('%s (score = %.5f)' % (human_string, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
